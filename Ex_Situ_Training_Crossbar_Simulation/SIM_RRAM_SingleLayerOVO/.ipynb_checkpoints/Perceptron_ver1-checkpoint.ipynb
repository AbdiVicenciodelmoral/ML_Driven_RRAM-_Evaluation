{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddeb1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35cab8",
   "metadata": {},
   "source": [
    "Algorithm 5 PerceptronTrain(D, MaxIter)\n",
    "\n",
    "1: w_d ← random, for all d = 1 . . . D // initialize weights\n",
    "\n",
    "2: b ← 0 // initialize bias\n",
    "3: for iter = 1 . . . MaxIter do\n",
    "4: for all (x,y) ∈ D do\n",
    "5: a ← ∑^D_d=1 w_d x_d + b // compute activation for this example\n",
    "6: if ya ≤ 0 then\n",
    "7: wd ← w_d + yx_d\n",
    ", for all d = 1 . . . D // update weights\n",
    "8: b ← b + y // update bias\n",
    "9: end if\n",
    "10: end for\n",
    "11: end for\n",
    "12: return w0, w1\n",
    ", . . . , wD, b\n",
    "\n",
    "\n",
    "\n",
    "Algorithm 6 PerceptronTest(w0, w1, . . . , w_D, b, xˆ)\n",
    "1: a ← ∑^D_d=1 w_d xˆ_d + b // compute activation for the test example\n",
    "2: return sign(a)\n",
    "\n",
    "\n",
    "b ← 0: This line initializes the bias term to zero. The bias is a constant that allows the perceptron to fit data that is not centered around zero.\n",
    "\n",
    "for iter = 1 . . . MaxIter do: The algorithm iterates up to MaxIter times, where MaxIter is a predefined maximum number of iterations. Each iteration is a full pass through the training dataset.\n",
    "\n",
    "for all (x,y) ∈ D do: For each iteration, the algorithm loops over all examples (x, y) in the dataset D, where x is the input vector and y is the corresponding target output.\n",
    "\n",
    "a ← ∑^D_d=1 w_d x_d + b: This line computes the weighted sum of the inputs plus the bias. This sum is also called the activation of the perceptron.\n",
    "\n",
    "if ya ≤ 0 then: If the product of the target output y and the activation a is less than or equal to zero, then the perceptron has made a mistake on this example, and the weights and bias need to be updated.\n",
    "\n",
    "wd ← w_d + yx_d, for all d = 1 . . . D: For each weight w_d, the algorithm adds the product of the target output y and the corresponding input x_d to the current weight. This is the weight update rule of the perceptron.\n",
    "\n",
    "b ← b + y: The bias is updated by adding the target output y to the current bias.\n",
    "\n",
    "end if: This ends the if-statement.\n",
    "\n",
    "end for: This ends the loop over all examples in the dataset.\n",
    "\n",
    "end for: This ends the loop over all iterations.\n",
    "\n",
    "return w0, w1, . . . , wD, b: The algorithm returns the final weights and bias after training.\n",
    "\n",
    "For the testing algorithm:\n",
    "\n",
    "a ← ∑^D_d=1 w_d xˆ_d + b: The activation for the test example xˆ is computed in the same way as for the training examples.\n",
    "\n",
    "return sign(a): The function returns the sign of the activation. If the activation is positive, the output is +1; if it's negative, the output is -1. This is because the single-layer perceptron is a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff558714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, data, output_dim, hidden_layers, hidden_dim, epochs, learning_rate):\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        # Access the data arrays\n",
    "        self.x_train = data['x_train']\n",
    "        self.y_train = data['y_train']\n",
    "        self.x_test = data['x_test']\n",
    "        self.y_test = data['y_test']\n",
    "        self.input_dim = len(self.x_train[0])\n",
    "    \n",
    "    \n",
    "    # Takes a tag, to specify which single class the perceptron will predict\n",
    "    def process_data(self,tag):\n",
    "        self.y_train = [1 if label == tag else -1 for label in self.y_train]\n",
    "        self.y_test = [1 if label == tag else -1 for label in self.y_test]    \n",
    "        # Convert input_vectors and binary_labels into NumPy arrays\n",
    "        input_vectors = np.array(self.x_train, dtype=np.float32)\n",
    "        self.y_train = np.array(self.y_train, dtype=np.float32)\n",
    "        \n",
    "        test_vectors = np.array(self.x_test, dtype=np.float32)\n",
    "        self.y_test = np.array(self.y_test, dtype=np.float32)\n",
    "\n",
    "        # Flatten X Vectors\n",
    "        self.x_train = input_vectors.reshape(-1, self.input_dim)\n",
    "        self.x_test = test_vectors.reshape(-1, 100)\n",
    "\n",
    "    #def intialize_Weights(self):\n",
    "    #    self.weights = np.random.uniform(-1, 1, self.input_dim)\n",
    "        \n",
    "    def activation_Function(self,a):\n",
    "        # Sigmoid Activation Function\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def predict(self,x_vector):\n",
    "        #1: a ← ∑^D_d=1 w_d xˆ_d + b // compute activation for the test example\n",
    "        a = np.dot(x_vector, self.weights) + self.bias\n",
    "        #a = self.activation_Function(a)  # apply sigmoid activation function\n",
    "        #2: return sign(a)\n",
    "        #if a <= 0.5:  # For sigmoid: we can set any threshold you like, 0.5 is commonly used\n",
    "        if a <= 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        total_size = len(self.x_train)\n",
    "        #1: w_d ← random, for all d = 1 . . . D // initialize weights\n",
    "        self.weights = np.random.uniform(-1, 1, self.input_dim)\n",
    "        #2: b ← 0 // initialize bias\n",
    "        self.bias = 0    \n",
    "        #3: for iter = 1 . . . MaxIter do\n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            correct_count = 0\n",
    "            #4: for all (x,y) ∈ D do\n",
    "            for x_vector,y in zip(self.x_train,self.y_train):  \n",
    "                #5: a ← ∑^D_d=1 w_d x_d + b // compute activation for this example\n",
    "                a = np.dot(x_vector, self.weights) + self.bias\n",
    "                #6: if ya ≤ 0 then\n",
    "                if y*a <= 0:\n",
    "                    #7: wd ← w_d + yx_d, for all d = 1 . . . D // update weights\n",
    "                    self.weights = self.weights + self.learning_rate * y * x_vector\n",
    "                    #8: b ← b + y // update bias\n",
    "                    self.bias += self.learning_rate * y\n",
    "                else:\n",
    "                    correct_count += 1\n",
    "            \n",
    "            end_time = time.time()\n",
    "            epoch_time = end_time - start_time\n",
    "            accuracy = correct_count / total_size\n",
    "            sys.stdout.write(\"\\rEpoch {}/{} - {:.2f}s - accuracy: {:.4f}\".format(epoch + 1, self.epochs, epoch_time, accuracy))\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "947eaafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = np.load('mnist_downsampled.npz')\n",
    "\n",
    "SLP = Perceptron(data, 1, 0, 0, 100, 0.01)\n",
    "SLP.process_data(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "883caf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,  41.,  21.,   0.],\n",
       "       [  0.,   0.,  25., ..., 133.,  40.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0., 135., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  96., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLP.x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a62d7ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLP.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4038f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 - 0.22s - accuracy: 0.0815"
     ]
    }
   ],
   "source": [
    "SLP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ba299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecceccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
