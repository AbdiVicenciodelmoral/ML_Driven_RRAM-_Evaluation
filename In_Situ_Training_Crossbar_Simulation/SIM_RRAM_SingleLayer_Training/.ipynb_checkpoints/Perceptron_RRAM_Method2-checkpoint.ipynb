{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e07da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9bb1d",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "\n",
    "## Method 1:\n",
    "\n",
    "A single-layer perceptron is a type of artificial neural network that uses a set of weights and a bias to make a binary decision.\n",
    "\n",
    "In the context of a 10x10 RRAM (Resistive Random Access Memory) crossbar being used as a single layer perceptron, each memristor in the crossbar can be seen as a weight of the perceptron. The state of each memristor (resistance level) can be modified by applying a SET or RESET voltage.\n",
    "\n",
    "Training a single-layer perceptron involves the following steps:\n",
    "\n",
    "1. Initialization: In the beginning, the weights (memristors in this case) are initialized. You can use different strategies to initialize the weights, including setting all to a uniform value, assigning small random values, or anything else that fits your context.\n",
    "\n",
    "    - First, the precision of synaptic weight is limited because the total plasticity range Gmax/Gmin is finite and precise control on the weight value is governed by the integer number of training pulses. The discrete weight values in this device are bounded between Gmax and Gmin. \n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "2. Forward Propagation: For each training example, the inputs are multiplied by their corresponding weights (memristor resistances). The results are then summed up, and a bias is added to the sum. This result is then passed through an activation function (for instance, a step function), which will output a binary value (e.g., 0 or 1).\n",
    "\n",
    "    -Input Application:\n",
    "\n",
    "    --Let's assume each pixel in your 10x10 image is represented by a grayscale intensity value between 0 and 1. Flatten the 10x10 image into a 1-dimensional array of 100 pixel intensity values. This is your input vector, let's denote it as x = [x1, x2, ..., x100].\n",
    "    Each of these input values will be connected to a corresponding memristor in the 10x10 RRAM crossbar.\n",
    "    Weighted Sum Calculation:\n",
    "\n",
    "    --Each memristor in the RRAM has a resistance value that corresponds to the weight of the connection in the perceptron model. Let's denote the weight vector as w = [w1, w2, ..., w100].\n",
    "    Calculate the product of each input value and its corresponding weight. This can be represented as y = [x1*w1, x2*w2, ..., x100*w100].\n",
    "    Summation and Bias:\n",
    "\n",
    "    --Sum up the values in the y vector to get a single value Z. This can be represented mathematically as Z = Σ (xi * wi).\n",
    "    Add a bias term b to this sum: Z = Z + b. The bias term is an additional parameter in the model that allows for more flexibility in the decision boundary.\n",
    "    Activation Function:\n",
    "\n",
    "    --The resulting Z is then passed through an activation function. This function is responsible for transforming the input signal into an output signal and it adds non-linearity to the model.\n",
    "    \n",
    "    ---In a binary classification problem like ours (recognizing whether the digit is '3' or not), a common choice would be the step function:\n",
    "    \n",
    "    ----If Z >= threshold, output 1 (meaning the image is recognized as '3')\n",
    "    \n",
    "    ----If Z < threshold, output 0 (meaning the image is not recognized as '3')\n",
    "    \n",
    "    ----The threshold in the step function is a parameter that you can tune.\n",
    "\n",
    "    3. Backward Propagation: The output from the forward propagation is compared with the actual output. If the output is correct, no changes are made. If it's incorrect, the weights (memristor states) need to be updated. This is where the SET and RESET voltages come in.\n",
    "\n",
    "    In memristive devices like an RRAM, the resistance can be modified by applying voltages. The SET voltage decreases the resistance (increases the conductance) while the RESET voltage increases the resistance (decreases the conductance).\n",
    "\n",
    "    For weight adjustment, if the perceptron incorrectly classifies an instance, you'd want to decrease the resistance (by applying a SET voltage) for the memristors corresponding to input features that would push the decision in the correct direction, and increase the resistance (by applying a RESET voltage) for the memristors corresponding to input features that would push the decision in the wrong direction.\n",
    "\n",
    "4. Repeat: Steps 2 and 3 are repeated for each instance in the training set. This process might be repeated for several epochs (an epoch is one pass through the entire training set) until the perceptron correctly classifies a desired amount of instances.\n",
    "\n",
    "5. Evaluation: After the training, you would typically evaluate the perceptron on a separate test set to see how well it generalizes to unseen data.\n",
    "\n",
    "\n",
    "\n",
    "## Method 2:\n",
    "\n",
    "## Deep Neural Network Optimized to Resistive Memory with\n",
    "Nonlinear Current-Voltage Characteristics\n",
    "\n",
    "Weight Mapping\n",
    "ANN utilizes a vector-matrix multiplication of the corresponding input vector and weight matrix\n",
    "to obtain a weighted sum for a layer as\n",
    "s = x · W (4)\n",
    "with s as the weighted sum vector, x as the input vector, and W as the weight matrix. To\n",
    "implement Eq. (4) using an emerging NVM crossbar array, each weight in particular row and\n",
    "column of the weight matrix must be mapped to a characteristic parameter of a corresponding\n",
    "device in the crossbar array. In previous approaches [4, 16, 23] which used RRAM crossbar arrays, weights were mapped to the conductance of the devices assuming linear I-V characteristics\n",
    "as follows,\n",
    "G(w) = (Gmax − Gmin)(w − wmax)\n",
    "wmax − wmin\n",
    "+\n",
    "Gmax(wmax − wmin)\n",
    "Gmax − Gmin\n",
    "\n",
    "\n",
    "We may need to scale the weight.\n",
    "\n",
    "### Implementation of multilayer perceptron network with highly uniform passive memristive crossbar circuits\n",
    "\n",
    "\n",
    "\n",
    "In our first set of experiments, the multilayer perceptron was trained ex-situ by first finding the synaptic weights in the software-implemented network, and then importing the weights into the hardware. Because of limited size of the classifier, we have used custom 4-class benchmark, which is comprised of a total of 40 training (Fig. 4d) and 640 test (Supplementary Fig. 4) 4 × 4-pixel black and white patterns representing stylized letters “A”, “T”, “V”, and “X”. As Supplementary Fig. 5 shows, the classes of the patterns in the benchmark are not linearly separable and the use of multi-bit (analog) weights significantly improve performance for the implemented training algorithm.\n",
    "\n",
    "In particular, the software-based perceptron was trained using conventional batch-mode backpropagation algorithm with mean-square error cost function. The neuron activation function was approximated with tangent hyperbolic with a slope specific to the hardware implementation. We assumed a linear I–V characteristics for the memristors, which is a good approximation for the considered range of voltages used for inference operation (Fig. 1c). During the training the weights were clipped within (10 μS, 100 μS) conductance range, which is an optimal range for the considered memristors.\n",
    "\n",
    "In addition, two different approaches for modeling weights were considered in the software network. In the simplest, hardware-oblivious approach, all memristors were assumed to be perfectly functional, while in a more advanced, hardware-aware approach, the software model utilized additional information about the defective memristors. These were the devices whose conductances were experimentally found to be stuck at some values, and hence could not be changed during tuning.\n",
    "\n",
    "The calculated synaptic weights were imported into the hardware by tuning memristors’ conductances to the desired values using an automated write-and-verify algorithm48. The stuck devices were excluded from tuning for the hardware-aware training approach. To speed up weight import, the maximum tuning error was set to 30% of the target conductance (Fig. 5a, b), which is adequate import precision for the considered benchmark according to the simulation results (Supplementary Fig. 5). Even though tuning accuracy was often worse than 30%, the weight errors were much smaller and, e.g., within 30% for 42 weights (out of 44 total) in the second layer of the network (Supplementary Fig. 6). This is due to our differential synapses implementation, in which one of the conductances was always selected to have the smallest (i.e., 10 µS) value and the cruder accuracy was used for tuning these devices because of their insignificant contribution to the actual weight.\n",
    "\n",
    "Fig. 5\n",
    "figure 5\n",
    "Ex-situ training experimental results. a, b The normalized difference between the target and the actual conductances after tuning in a the first and b the second layer of the network for the hardware-oblivious training approach; c Time response of the trained network for 6 different input patterns, in particular showing less than 5 μs propagation delay. Perceptron output voltage for d, f hardware-oblivious and e, g hardware-aware ex-situ training approaches, with d-g panels showing measured results for training/test patterns\n",
    "\n",
    "Full size image\n",
    "After weight import had been completed, the inference was performed by applying ±0.2 V inputs specific to the pattern pixels and measuring four analog voltage outputs. Figure 5c shows typical transient response. Though the developed system was not optimized for speed, the experimentally measured classification rate was quite high—about 300,000 patterns per second and was mainly limited by the chip-to-chip propagation delay of analog signals on the printed circuit board.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3127e",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "1. Set up the environment\n",
    "\n",
    "- Begin by importing the necessary libraries in Python such as NumPy for numerical calculations, and sklearn (or any other machine learning library) for training a Perceptron.\n",
    "\n",
    "2. Pre-training the Perceptron\n",
    "\n",
    "- Use sklearn or any other library to pre-train a Perceptron model on a given dataset. Save the weights of the model after training. These weights will serve as your 'A' matrix for the memristor crossbar simulation.\n",
    "\n",
    "3. Calculate GIDEAL\n",
    "\n",
    "Write a function to transform the 'A' matrix (the Perceptron weights) into GIDEAL, which represents the ideal conductances in the memristor crossbar array. You will need to implement the equations provided in the paper (Equation 1 and 2) to perform this transformation.\n",
    "\n",
    "Calculate the ideal current\n",
    "\n",
    "Write a function to calculate the ideal currents that would flow through the memristors in the crossbar array if the conductances were set to GIDEAL and given the input voltage vector. This would require matrix-vector multiplication as described in the previous answer.\n",
    "\n",
    "Tuning the simulated crossbar\n",
    "\n",
    "Implement the tuning process to adjust the conductances in the simulated crossbar so that the resulting currents match the ideal currents. You'll need to simulate the non-ideal effects of the crossbar and adjust the conductances to compensate for these effects.\n",
    "\n",
    "Analyzing the updated conductances\n",
    "\n",
    "After the tuning process, analyze the final conductances of the memristors in the simulated crossbar. This could include statistical analysis of the conductances, comparisons between the final and initial conductances, or comparisons between the final conductances and GIDEAL.\n",
    "\n",
    "Performance evaluation\n",
    "\n",
    "To evaluate the performance of the simulated memristor crossbar array, you could try to make predictions using the final conductances as weights in a Perceptron model. Compare the performance of this model (using metrics like accuracy or mean squared error) with the original pre-trained Perceptron model.\n",
    "\n",
    "Further analysis\n",
    "\n",
    "Depending on the results, you might want to explore further aspects of the simulation, such as the effects of different tuning methods, the influence of different non-ideal effects, or the performance of the simulation with different types of matrices (not just Perceptron weights).\n",
    "\n",
    "Remember to keep good documentation of your process, and consider visualizing your data and results along the way. It's also important to thoroughly test each part of your workflow to ensure that your implementation is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b10e0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight Momentum:\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import resample\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, data, epochs, learning_rate):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate  # store the initial learning rate\n",
    "        self.x_train = self.normalize_STD(data['x_train']).reshape(data['x_train'].shape[0], -1)\n",
    "        self.y_train = data['y_train']\n",
    "        self.x_test = self.normalize_STD(data['x_test']).reshape(data['x_test'].shape[0], -1)\n",
    "        self.y_test = data['y_test']\n",
    "        self.input_dim = self.x_train.shape[1]\n",
    "        self.weights = np.random.uniform(-1, 1, self.input_dim)\n",
    "        self.bias = 0\n",
    "        self.delta_w = np.zeros(self.input_dim)  # track the last update for weights\n",
    "        self.delta_b = 0  # track the last update for bias\n",
    "\n",
    "\n",
    "    def process_data(self, tag):\n",
    "        self.y_train = np.array([1 if label == tag else 0 for label in self.y_train], dtype=np.float32)\n",
    "        self.y_test = np.array([1 if label == tag else 0 for label in self.y_test], dtype=np.float32)\n",
    "    \n",
    "\n",
    "    def balance_data(self):\n",
    "        # Separate majority and minority classes\n",
    "        x_zero = self.x_train[self.y_train == 0]\n",
    "        y_zero = self.y_train[self.y_train == 0]\n",
    "        x_non_zero = self.x_train[self.y_train != 0]\n",
    "        y_non_zero = self.y_train[self.y_train != 0]\n",
    "\n",
    "        # Upsample minority class\n",
    "        x_non_zero_upsampled, y_non_zero_upsampled = resample(x_non_zero,\n",
    "                                                              y_non_zero,\n",
    "                                                              replace=True, # sample with replacement\n",
    "                                                              n_samples=x_zero.shape[0], # to match majority class\n",
    "                                                              random_state=123) # reproducible results\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        self.x_train = np.vstack((x_zero, x_non_zero_upsampled))\n",
    "        self.y_train = np.hstack((y_zero, y_non_zero_upsampled))\n",
    "\n",
    "\n",
    "    \n",
    "    def normalize_STD(self, data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return (data - mean) / std\n",
    "    \n",
    "    def normalize_MIN_MAX(self, data):\n",
    "        return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    \n",
    "    \n",
    "    # Clipping to prevent overflow/underflow\n",
    "    \"\"\"def activation_Function(self, z):\n",
    "        f = 1 / (1 + np.exp(-z))\n",
    "        print(\"f: \",f)\n",
    "        return f\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"def activation_Function(self, z):\n",
    "        f = 1 / (1 + np.exp(-z + 1e-7))  # add a small constant to avoid nan values\n",
    "        #print(\"f: \",f)\n",
    "        return f\"\"\"\n",
    "    \n",
    "    def activation_Function(self, z):\n",
    "        z = np.clip(z, -500, 500)  # clip values to avoid overflow/underflow\n",
    "        f = 1 / (1 + np.exp(-z))\n",
    "        return f\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        a = self.activation_Function(z)\n",
    "        #print(a.shape)\n",
    "        #print(\"a: \",a)\n",
    "        return a\n",
    "    \n",
    "\n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def learning_rate_scheduler(self, current_epoch, decay_factor=0.1, decay_epochs=10):\n",
    "        if (current_epoch+1) % decay_epochs == 0:\n",
    "            self.learning_rate *= decay_factor\n",
    "            print(f\"Reduced learning rate to {self.learning_rate}\")\n",
    "        \n",
    "    def train(self, validation_data, batch_size=1, patience=5):\n",
    "        best_val_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        lambda_value = 0.001\n",
    "        momentum = 0.9  # choose a value for momentum\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Call learning rate scheduler at the start of each epoch\n",
    "            self.learning_rate_scheduler(epoch)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Shuffle the indices of the data\n",
    "            idx = np.random.permutation(len(self.x_train))\n",
    "            self.x_train, self.y_train = self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "            for i in range(0, len(self.x_train), batch_size):\n",
    "                \n",
    "                # Get the current batch\n",
    "                x_batch = self.x_train[i:i+batch_size]\n",
    "                y_batch = self.y_train[i:i+batch_size]\n",
    "\n",
    "                # Compute the predictions and error for this batch\n",
    "                a_batch = self.predict(x_batch)\n",
    "                \n",
    "                #print(a_batch.shape)\n",
    "                #print(\"a: \",a_batch)\n",
    "                #print(\"y: \",y_batch)\n",
    "                \n",
    "                # Compute the gradient of the binary cross-entropy loss\n",
    "                epsilon = 1e-7  # small constant\n",
    "                error_batch = -(y_batch/(a_batch + epsilon)) + ((1 - y_batch) / (1 - a_batch + epsilon))\n",
    "              \n",
    "                # Compute the weight and bias updates\n",
    "                delta_w_current = self.learning_rate * (np.mean(error_batch[:, np.newaxis] * x_batch, axis=0) + (lambda_value * self.weights))\n",
    "                delta_b_current = self.learning_rate * np.mean(error_batch)\n",
    "                \n",
    "                # Apply momentum to the weight and bias updates\n",
    "                delta_w = momentum * self.delta_w - delta_w_current\n",
    "                delta_b = momentum * self.delta_b - delta_b_current\n",
    "\n",
    "                # Update the weights and bias\n",
    "                self.weights += delta_w\n",
    "                self.bias += delta_b\n",
    "                \n",
    "                # Store the updates for the next iteration\n",
    "                self.delta_w = delta_w\n",
    "                self.delta_b = delta_b\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_time = end_time - start_time\n",
    "            a_train = self.predict(self.x_train)\n",
    "            train_loss = self.binary_cross_entropy(self.y_train, a_train)\n",
    "            train_accuracy = np.mean((a_train > 0.5) == (self.y_train == 1))\n",
    "\n",
    "            # Validate the model\n",
    "            a_val = self.predict(validation_data['x_val'])\n",
    "            val_loss = self.binary_cross_entropy(validation_data['y_val'], a_val)\n",
    "            val_accuracy = np.mean((a_val > 0.5) == (validation_data['y_val'] == 1))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} - {epoch_time:.2f}s - loss: {train_loss:.4f} - accuracy: {train_accuracy:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Check if we need to stop training early\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0  # Reset the counter\n",
    "            else:\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping due to validation loss not improving\")\n",
    "                    break\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4293b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: (48000, 10, 10)\n",
      "Downsampled size: (48000, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import cv2\n",
    "def downsample_image(image, size):\n",
    "    return cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Define the target size (10x10)\n",
    "target_size = (10, 10)\n",
    "\n",
    "# Downsample the training and test images\n",
    "x_train = np.array([downsample_image(img, target_size) for img in x_train])\n",
    "x_test= np.array([downsample_image(img, target_size) for img in x_test])\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the data\n",
    "np.savez('mnist_downsampled.npz', x_train=x_train, y_train = y_train, x_val=x_val, y_val=y_val, x_test=x_test, y_test =y_test)\n",
    "\n",
    "# Verify the downsampling by printing the shape\n",
    "print(\"Original size:\", x_train.shape)\n",
    "print(\"Downsampled size:\", x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ea6c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.load('mnist_downsampled.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2eec218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_load = np.load('mnist_downsampled.npz')\\ndata = {}\\n# Training data\\ntrain_mask = np.isin(data_load['y_train'], [1, 3])\\nx_train = data_load['x_train'][train_mask]\\ny_train = data_load['y_train'][train_mask]\\n\\n# Testing data\\ntest_mask = np.isin(data_load['y_test'], [1, 3])\\nx_test = data_load['x_test'][test_mask]\\ny_test = data_load['y_test'][test_mask]\\n\\n# Validation data\\nval_mask = np.isin(data_load['y_val'], [1, 3])\\nx_val = data_load['x_val'][val_mask]\\ny_val = data_load['y_val'][val_mask]\\n\\ndata['x_train'] = x_train\\ndata['x_val'] = x_val\\ndata['y_train'] = y_train\\ndata['y_val'] = y_val\\ndata['x_test'] = x_test\\ndata['y_test'] = y_test\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "\"\"\"data_load = np.load('mnist_downsampled.npz')\n",
    "data = {}\n",
    "# Training data\n",
    "train_mask = np.isin(data_load['y_train'], [1, 3])\n",
    "x_train = data_load['x_train'][train_mask]\n",
    "y_train = data_load['y_train'][train_mask]\n",
    "\n",
    "# Testing data\n",
    "test_mask = np.isin(data_load['y_test'], [1, 3])\n",
    "x_test = data_load['x_test'][test_mask]\n",
    "y_test = data_load['y_test'][test_mask]\n",
    "\n",
    "# Validation data\n",
    "val_mask = np.isin(data_load['y_val'], [1, 3])\n",
    "x_val = data_load['x_val'][val_mask]\n",
    "y_val = data_load['y_val'][val_mask]\n",
    "\n",
    "data['x_train'] = x_train\n",
    "data['x_val'] = x_val\n",
    "data['y_train'] = y_train\n",
    "data['y_val'] = y_val\n",
    "data['x_test'] = x_test\n",
    "data['y_test'] = y_test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bfd7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLP = Perceptron(data, 100, 0.01)\n",
    "SLP.process_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c6a534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - 3.76s - loss: 1.1132 - accuracy: 0.9309 - val_loss: 1559.4311 - val_accuracy: 0.5207\n",
      "Epoch 2/100 - 3.29s - loss: 0.6719 - accuracy: 0.9583 - val_loss: 1045.3351 - val_accuracy: 0.6771\n",
      "Epoch 3/100 - 2.88s - loss: 0.5148 - accuracy: 0.9681 - val_loss: 988.5631 - val_accuracy: 0.6902\n",
      "Epoch 4/100 - 2.88s - loss: 0.7488 - accuracy: 0.9535 - val_loss: 472.8432 - val_accuracy: 0.7918\n",
      "Epoch 5/100 - 2.97s - loss: 1.2656 - accuracy: 0.9215 - val_loss: 1820.2098 - val_accuracy: 0.5316\n",
      "Epoch 6/100 - 3.08s - loss: 0.7068 - accuracy: 0.9561 - val_loss: 1149.2310 - val_accuracy: 0.6353\n",
      "Epoch 7/100 - 3.31s - loss: 0.8543 - accuracy: 0.9470 - val_loss: 1348.6226 - val_accuracy: 0.6110\n",
      "Epoch 8/100 - 2.94s - loss: 0.7898 - accuracy: 0.9510 - val_loss: 1120.6845 - val_accuracy: 0.7131\n",
      "Epoch 9/100 - 2.95s - loss: 1.1649 - accuracy: 0.9277 - val_loss: 1805.7761 - val_accuracy: 0.5364\n",
      "Reduced learning rate to 0.001\n",
      "Epoch 10/100 - 2.96s - loss: 0.4973 - accuracy: 0.9691 - val_loss: 500.6308 - val_accuracy: 0.7952\n",
      "Epoch 11/100 - 3.04s - loss: 0.7428 - accuracy: 0.9539 - val_loss: 1003.6053 - val_accuracy: 0.7027\n",
      "Epoch 12/100 - 3.86s - loss: 0.5255 - accuracy: 0.9674 - val_loss: 769.4000 - val_accuracy: 0.7753\n",
      "Epoch 13/100 - 3.09s - loss: 0.5054 - accuracy: 0.9686 - val_loss: 616.0309 - val_accuracy: 0.7565\n",
      "Epoch 14/100 - 3.27s - loss: 0.8489 - accuracy: 0.9473 - val_loss: 1065.5418 - val_accuracy: 0.7231\n",
      "Epoch 15/100 - 3.11s - loss: 0.9688 - accuracy: 0.9399 - val_loss: 814.6650 - val_accuracy: 0.7625\n",
      "Epoch 16/100 - 3.35s - loss: 0.7847 - accuracy: 0.9513 - val_loss: 1069.4034 - val_accuracy: 0.6975\n",
      "Epoch 17/100 - 3.61s - loss: 0.6441 - accuracy: 0.9600 - val_loss: 714.2210 - val_accuracy: 0.7571\n",
      "Epoch 18/100 - 3.09s - loss: 0.9446 - accuracy: 0.9414 - val_loss: 1478.4431 - val_accuracy: 0.6217\n",
      "Epoch 19/100 - 3.09s - loss: 0.5507 - accuracy: 0.9658 - val_loss: 1553.6327 - val_accuracy: 0.5654\n",
      "Reduced learning rate to 0.0001\n",
      "Epoch 20/100 - 3.13s - loss: 0.4869 - accuracy: 0.9698 - val_loss: 700.8591 - val_accuracy: 0.7498\n",
      "Epoch 21/100 - 3.43s - loss: 0.5541 - accuracy: 0.9656 - val_loss: 650.5371 - val_accuracy: 0.7545\n",
      "Epoch 22/100 - 3.54s - loss: 0.4893 - accuracy: 0.9696 - val_loss: 492.0707 - val_accuracy: 0.7873\n",
      "Epoch 23/100 - 3.09s - loss: 1.1021 - accuracy: 0.9316 - val_loss: 565.1958 - val_accuracy: 0.7836\n",
      "Epoch 24/100 - 3.10s - loss: 0.5890 - accuracy: 0.9635 - val_loss: 409.1243 - val_accuracy: 0.8066\n",
      "Epoch 25/100 - 3.08s - loss: 0.5480 - accuracy: 0.9660 - val_loss: 461.6357 - val_accuracy: 0.8023\n",
      "Epoch 26/100 - 3.54s - loss: 0.5792 - accuracy: 0.9641 - val_loss: 394.6811 - val_accuracy: 0.8102\n",
      "Epoch 27/100 - 3.44s - loss: 1.0940 - accuracy: 0.9321 - val_loss: 818.7361 - val_accuracy: 0.7206\n",
      "Epoch 28/100 - 3.10s - loss: 0.7142 - accuracy: 0.9557 - val_loss: 393.5703 - val_accuracy: 0.8103\n",
      "Epoch 29/100 - 3.07s - loss: 0.6259 - accuracy: 0.9612 - val_loss: 468.1232 - val_accuracy: 0.7922\n",
      "Reduced learning rate to 1e-05\n",
      "Epoch 30/100 - 3.15s - loss: 0.6524 - accuracy: 0.9595 - val_loss: 358.1777 - val_accuracy: 0.8191\n",
      "Epoch 31/100 - 3.77s - loss: 0.7075 - accuracy: 0.9561 - val_loss: 239.6721 - val_accuracy: 0.8484\n",
      "Epoch 32/100 - 3.44s - loss: 0.5665 - accuracy: 0.9649 - val_loss: 202.6031 - val_accuracy: 0.8576\n",
      "Epoch 33/100 - 3.19s - loss: 0.6310 - accuracy: 0.9609 - val_loss: 249.0877 - val_accuracy: 0.8461\n",
      "Epoch 34/100 - 3.52s - loss: 0.5353 - accuracy: 0.9668 - val_loss: 147.3557 - val_accuracy: 0.8712\n",
      "Epoch 35/100 - 3.44s - loss: 0.5453 - accuracy: 0.9662 - val_loss: 155.4335 - val_accuracy: 0.8692\n",
      "Epoch 36/100 - 3.78s - loss: 0.5592 - accuracy: 0.9653 - val_loss: 144.6452 - val_accuracy: 0.8719\n",
      "Epoch 37/100 - 3.14s - loss: 0.6659 - accuracy: 0.9587 - val_loss: 173.6335 - val_accuracy: 0.8648\n",
      "Epoch 38/100 - 3.15s - loss: 0.6578 - accuracy: 0.9592 - val_loss: 154.4356 - val_accuracy: 0.8695\n",
      "Epoch 39/100 - 3.15s - loss: 0.5507 - accuracy: 0.9658 - val_loss: 150.0393 - val_accuracy: 0.8706\n",
      "Reduced learning rate to 1.0000000000000002e-06\n",
      "Epoch 40/100 - 3.19s - loss: 0.5708 - accuracy: 0.9646 - val_loss: 151.7250 - val_accuracy: 0.8702\n",
      "Epoch 41/100 - 3.74s - loss: 0.6571 - accuracy: 0.9592 - val_loss: 144.9823 - val_accuracy: 0.8718\n",
      "Epoch 42/100 - 3.11s - loss: 0.5833 - accuracy: 0.9638 - val_loss: 138.9138 - val_accuracy: 0.8733\n",
      "Epoch 43/100 - 3.12s - loss: 0.5803 - accuracy: 0.9640 - val_loss: 129.8246 - val_accuracy: 0.8756\n",
      "Epoch 44/100 - 3.11s - loss: 0.6142 - accuracy: 0.9619 - val_loss: 137.2281 - val_accuracy: 0.8738\n",
      "Epoch 45/100 - 3.28s - loss: 0.6239 - accuracy: 0.9613 - val_loss: 129.4874 - val_accuracy: 0.8757\n",
      "Epoch 46/100 - 3.64s - loss: 0.5806 - accuracy: 0.9640 - val_loss: 119.0335 - val_accuracy: 0.8782\n",
      "Epoch 47/100 - 3.52s - loss: 0.6333 - accuracy: 0.9607 - val_loss: 122.4049 - val_accuracy: 0.8774\n",
      "Epoch 48/100 - 3.44s - loss: 0.6132 - accuracy: 0.9619 - val_loss: 118.0221 - val_accuracy: 0.8785\n",
      "Epoch 49/100 - 3.58s - loss: 0.6481 - accuracy: 0.9598 - val_loss: 120.7192 - val_accuracy: 0.8778\n",
      "Reduced learning rate to 1.0000000000000002e-07\n",
      "Epoch 50/100 - 4.24s - loss: 0.6316 - accuracy: 0.9608 - val_loss: 118.6964 - val_accuracy: 0.8783\n",
      "Epoch 51/100 - 3.32s - loss: 0.6165 - accuracy: 0.9617 - val_loss: 117.6850 - val_accuracy: 0.8786\n",
      "Epoch 52/100 - 3.37s - loss: 0.6158 - accuracy: 0.9618 - val_loss: 118.3592 - val_accuracy: 0.8784\n",
      "Epoch 53/100 - 3.28s - loss: 0.6394 - accuracy: 0.9603 - val_loss: 121.0563 - val_accuracy: 0.8778\n",
      "Epoch 54/100 - 3.22s - loss: 0.6111 - accuracy: 0.9621 - val_loss: 117.3478 - val_accuracy: 0.8787\n",
      "Epoch 55/100 - 3.73s - loss: 0.5913 - accuracy: 0.9633 - val_loss: 115.9993 - val_accuracy: 0.8790\n",
      "Epoch 56/100 - 3.20s - loss: 0.6045 - accuracy: 0.9625 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 57/100 - 3.13s - loss: 0.6158 - accuracy: 0.9618 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 58/100 - 3.19s - loss: 0.6222 - accuracy: 0.9614 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 59/100 - 3.35s - loss: 0.6202 - accuracy: 0.9615 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Reduced learning rate to 1.0000000000000004e-08\n",
      "Epoch 60/100 - 3.55s - loss: 0.6182 - accuracy: 0.9616 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 61/100 - 3.16s - loss: 0.6148 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 62/100 - 3.44s - loss: 0.6151 - accuracy: 0.9618 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 63/100 - 3.50s - loss: 0.6126 - accuracy: 0.9620 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 64/100 - 6.62s - loss: 0.6146 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 65/100 - 3.89s - loss: 0.6145 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 66/100 - 3.58s - loss: 0.6145 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 67/100 - 3.61s - loss: 0.6141 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 68/100 - 4.25s - loss: 0.6140 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 69/100 - 3.50s - loss: 0.6132 - accuracy: 0.9620 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Reduced learning rate to 1.0000000000000005e-09\n",
      "Epoch 70/100 - 3.19s - loss: 0.6133 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 71/100 - 3.28s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 72/100 - 3.90s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 73/100 - 3.27s - loss: 0.6142 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 74/100 - 3.56s - loss: 0.6146 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 75/100 - 3.11s - loss: 0.6145 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 76/100 - 3.11s - loss: 0.6144 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 77/100 - 3.85s - loss: 0.6141 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 78/100 - 3.11s - loss: 0.6135 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 79/100 - 3.09s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Reduced learning rate to 1.0000000000000006e-10\n",
      "Epoch 80/100 - 3.17s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 - 3.31s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 82/100 - 3.84s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 83/100 - 3.09s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 84/100 - 3.14s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 85/100 - 3.12s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 86/100 - 3.29s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 87/100 - 3.61s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 88/100 - 3.13s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 89/100 - 3.09s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Reduced learning rate to 1.0000000000000006e-11\n",
      "Epoch 90/100 - 3.12s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 91/100 - 3.46s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 92/100 - 3.56s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 93/100 - 3.11s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 94/100 - 3.18s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 95/100 - 3.15s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 96/100 - 3.50s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 97/100 - 3.64s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 98/100 - 3.31s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Epoch 99/100 - 3.57s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n",
      "Reduced learning rate to 1.0000000000000006e-12\n",
      "Epoch 100/100 - 3.53s - loss: 0.6138 - accuracy: 0.9619 - val_loss: 117.0107 - val_accuracy: 0.8788\n"
     ]
    }
   ],
   "source": [
    "SLP.train({'x_val': data['x_val'].reshape(data['x_val'].shape[0], -1), 'y_val': data['y_val']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ef20428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.34628230e+06 -3.34628232e+06 -3.35808281e+06 -3.36709700e+06\n",
      " -3.14715952e+06 -2.98900353e+06 -3.16647733e+06 -3.33799843e+06\n",
      " -3.34628282e+06 -3.34628231e+06 -3.34246470e+06 -1.51141897e+06\n",
      "  2.09257183e+05 -3.39460851e+04  4.40743452e+03  3.40251514e+03\n",
      " -7.03127136e+03 -8.28997664e+03 -2.42054572e+06 -3.34869635e+06\n",
      " -3.27536747e+06  1.64760265e+05 -1.79138513e+04  2.55827493e+04\n",
      "  1.49532378e+04  2.12354321e+04  6.86363361e+03  1.17917855e+04\n",
      " -1.66186977e+05 -3.41561974e+06 -3.16064528e+06  7.43424741e+03\n",
      "  7.28438120e+03 -6.27006661e+04 -3.73113128e+04  3.75363617e+04\n",
      "  3.80605225e+04  6.09784010e+04 -7.38322081e+04 -3.39122403e+06\n",
      " -3.36503135e+06 -6.88549768e+04 -5.02743841e+04 -4.67226101e+04\n",
      "  2.33161455e+04  6.13603289e+04  1.66174922e+04 -1.43736073e+04\n",
      " -1.44463107e+06 -3.30248885e+06 -2.71667683e+06 -8.61632269e+03\n",
      " -6.02081184e+04 -3.03359776e+04  2.37045442e+04  5.73143766e+03\n",
      " -1.61127412e+04  1.18655516e+04  1.53727294e+05 -3.49349137e+06\n",
      " -1.79837879e+06  1.25047468e+05 -2.51910875e+04 -9.34959607e+04\n",
      " -9.22205535e+04 -5.53369652e+04  3.34859618e+04  4.25772319e+04\n",
      " -2.27484473e+04 -3.84994105e+06 -1.83461475e+06  1.49451487e+05\n",
      " -2.58206496e+04  5.47195119e+03 -1.63528077e+04  1.60938777e+04\n",
      "  3.33754476e+03 -1.40748549e+03 -3.81148121e+04 -3.42870275e+06\n",
      " -2.85996598e+06  1.72688906e+05  4.46427168e+04 -5.86387677e+03\n",
      " -9.60280491e+03 -2.40836249e+04  2.46486513e+04  4.26255018e+04\n",
      " -2.16934999e+06 -3.34947596e+06 -3.34231263e+06 -2.81018261e+06\n",
      " -4.50392961e+05  6.24106129e+04 -1.63859671e+04  1.53816755e+04\n",
      " -7.46449574e+05 -2.42606562e+06 -3.29147405e+06 -3.34628230e+06]\n",
      "209257.18274305642\n",
      "-3849941.0530521953\n"
     ]
    }
   ],
   "source": [
    "print(SLP.weights)\n",
    "print(max(SLP.weights))\n",
    "print(min(SLP.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90ad4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.21960784 0.47843137 0.02352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.61176471 0.6627451  0.00784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.87843137\n",
      " 0.48627451 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11372549 0.92156863 0.15686275 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.47058824 0.81960784 0.02745098 0.         0.         0.\n",
      " 0.         0.         0.         0.15686275 0.90980392 0.43529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.42352941 0.98431373 0.17254902 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.24313725\n",
      " 0.52156863 0.03137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "3849941.0530521953\n",
      "12.866666666666664\n",
      "49535908.21593823\n",
      "_________________\n",
      "y_corrected:  -105260469.84733713\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.44705882352941\n",
      "47920442.75446144\n",
      "_________________\n",
      "y_corrected:  -103645004.39511156\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "9.756862745098038\n",
      "37563346.43134847\n",
      "_________________\n",
      "y_corrected:  -93287908.12732069\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.717647058823527\n",
      "45112250.45694101\n",
      "_________________\n",
      "y_corrected:  -100836812.10974249\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "6.188235294117647\n",
      "23824341.104770057\n",
      "_________________\n",
      "y_corrected:  -79548902.87988704\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.831372549019605\n",
      "45550086.89042538\n",
      "_________________\n",
      "y_corrected:  -101274648.54033275\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.486274509803922\n",
      "44221479.781921096\n",
      "_________________\n",
      "y_corrected:  -99946041.43977562\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "24.55294117647059\n",
      "94527376.2084698\n",
      "_________________\n",
      "y_corrected:  -150251937.57593265\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "6.125490196078431\n",
      "23582776.175951093\n",
      "_________________\n",
      "y_corrected:  -79307337.95253296\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.960784313725489\n",
      "49898255.60916668\n",
      "_________________\n",
      "y_corrected:  -105622817.23399639\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.815686274509805\n",
      "49339636.71127284\n",
      "_________________\n",
      "y_corrected:  -105064198.33967945\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.20392156862745\n",
      "62384142.867496744\n",
      "_________________\n",
      "y_corrected:  -118108704.42257287\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "18.576470588235296\n",
      "71518316.73846373\n",
      "_________________\n",
      "y_corrected:  -127242878.23868573\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "7.215686274509803\n",
      "27779966.814180546\n",
      "_________________\n",
      "y_corrected:  -83504528.56649639\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "9.509803921568627\n",
      "36612184.52412382\n",
      "_________________\n",
      "y_corrected:  -92336746.2255184\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.458823529411763\n",
      "63365500.390823774\n",
      "_________________\n",
      "y_corrected:  -119090061.93761715\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.811764705882352\n",
      "53174479.95627385\n",
      "_________________\n",
      "y_corrected:  -108899041.56214628\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "5.92156862745098\n",
      "22797690.157289468\n",
      "_________________\n",
      "y_corrected:  -78522251.9383865\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.270588235294117\n",
      "43391100.33910592\n",
      "_________________\n",
      "y_corrected:  -99115662.00162789\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "8.435294117647057\n",
      "32475385.1180991\n",
      "_________________\n",
      "y_corrected:  -88199946.84648311\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.329411764705885\n",
      "62867272.72513468\n",
      "_________________\n",
      "y_corrected:  -118591834.27557287\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "14.09019607843137\n",
      "54246424.32790799\n",
      "_________________\n",
      "y_corrected:  -109970985.92750399\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "10.729411764705882\n",
      "41307602.82804237\n",
      "_________________\n",
      "y_corrected:  -97032164.50290845\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "17.48627450980392\n",
      "67321126.10023427\n",
      "_________________\n",
      "y_corrected:  -123045687.6248618\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "14.164705882352935\n",
      "54533282.68088049\n",
      "_________________\n",
      "y_corrected:  -110257844.27902524\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "8.584313725490196\n",
      "33049101.82404414\n",
      "_________________\n",
      "y_corrected:  -88773663.54608065\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "23.65098039215686\n",
      "91054880.3566972\n",
      "_________________\n",
      "y_corrected:  -146779441.74539635\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "17.211764705882352\n",
      "66264279.53665131\n",
      "_________________\n",
      "y_corrected:  -121988841.06785494\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.337254901960783\n",
      "43647763.07597606\n",
      "_________________\n",
      "y_corrected:  -99372324.73710555\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.533333333333333\n",
      "52102535.58463971\n",
      "_________________\n",
      "y_corrected:  -107827097.19748497\n",
      "y_train:  0.0\n",
      "_________________\n",
      "Accuracy: 0.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0, 0.0, 0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Crossbar():\n",
    "    def __init__(self,G,A,bias):\n",
    "        # Initialize empty crossbar array\n",
    "        self.Gon = np.max(G)\n",
    "        self.Goff = np.min(G)\n",
    "        self.A = A\n",
    "        self.bias = bias\n",
    "        self.Gideal = None\n",
    "        self.ASHIFT = None\n",
    "    \n",
    "   \n",
    "    def weight_mapping(self):\n",
    "        Amin = np.min(self.A)\n",
    "        Amax = np.max(self.A)\n",
    "        self.ASHIFT = -Amin if Amin < 0 else (0.01 if Amin == 0 else 0)\n",
    "        A_S = self.A + self.ASHIFT\n",
    "        a = (self.Gon - self.Goff )/(Amax - Amin)\n",
    "        b = self.Gon - a * (Amax)\n",
    "        self.Gideal = (a * A_S) + b\n",
    "\n",
    "    def process_data(self, y, tag):\n",
    "        return np.array([1 if label == tag else 0 for label in y], dtype=np.float32)\n",
    "     \n",
    "    def activation_Function(self, z):\n",
    "        z = np.clip(z, -500, 500)  # clip values to avoid overflow/underflow\n",
    "        f = 1 / (1 + np.exp(-z))\n",
    "        return f\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.Gideal) + self.bias\n",
    "        a = self.activation_Function(z)\n",
    "        return z\n",
    "    \n",
    "    def test_conductance(self,x_train,y_train,Vmax):\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        \"\"\"for i in range(10):\n",
    "            print(x_train[i])\"\"\"\n",
    "        for i in range(0, 30):\n",
    "        #for i in range(0, len(x_train)):\n",
    "            # Get the current batch\n",
    "            x = x_train[i]\n",
    "            y = y_train[i]\n",
    "            #print(x)\n",
    "            # Compute the predictions\n",
    "            Vin = x * Vmax\n",
    "            #print(\"VIN: \",Vin)\n",
    "            \n",
    "            #Simulated non-ideal behaviour here?????\n",
    "            \n",
    "            # And then predict?\n",
    "            a = self.predict(Vin)\n",
    "            print(self.ASHIFT)\n",
    "            print(sum(x))\n",
    "            print(self.ASHIFT*sum(x))\n",
    "            y_corrected = a - (self.ASHIFT*sum(x))\n",
    "            print(\"_________________\")\n",
    "            print(\"y_corrected: \",y_corrected)\n",
    "            print(\"y_train: \",y_train[i])\n",
    "            print(\"_________________\")\n",
    "            #y_corrected = a\n",
    "            # Update counts based on predictions\n",
    "            if y_corrected > 0.5:\n",
    "                if y_train[i] == 1:\n",
    "                    true_positive += 1\n",
    "                else:\n",
    "                    false_positive += 1\n",
    "            elif y_train[i] == 1:\n",
    "                false_negative += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n",
    "        accuracy = true_positive / len(x_train)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}\\nPrecision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1 Score: {f1_score:.2f}\")\n",
    "        return accuracy, precision, recall, f1_score\n",
    "    \n",
    "    \n",
    "# Known Conductance\n",
    "knc = additional_conductance_values = [\n",
    "    2.98e-3,802e-6,5.17e-3,3.07e-3,3.80e-3,740e-6,416.67e-6,2.04e-3,24.9e-6,240.57e-6,42.37e-6,\n",
    "    380e-6,3.07e-3,2.00e-3,1.03e-3,792e-6,260e-6,1.65e-3,2.88e-3,2.30e-3,2.01e-3,1.73e-3,3.55e-3,\n",
    "    4.03e-3,5.71e-3,3.36e-3, 301e-6,14.74e-3,16.75e-3,23.42e-3,758e-6,215.57e-6,2.08e-3,20.04e-6,\n",
    "    5.74e-3,184.7e-6,143.08e-6,0.416e-3,0.730e-3,1.22e-3,692.6e-6,478.03e-6,1.5e-3,3.97e-3,1.3e-3,\n",
    "    2.04e-3,1.67e-3,11.43e-3,5.007e-3,7.93e-3,\n",
    "    0.004125923175310475, 0.004606596646397641, 0.00484214603912454, 0.0017425549340442958, \n",
    "    0.0016838702072844226, 0.004572473708276178, 0.0004716981132075472, 0.0026288808854070824, \n",
    "    0.0053769222497042695, 0.0048652330446628395, 0.0057359183205231154, 0.009719117504130625, \n",
    "    0.0032565864460872116, 0.004441384823788057, 0.006468723720809884, 0.0001269035532994924, \n",
    "    0.0004484304932735426, 0.0006711409395973154, 0.00226510827217541, 0.0022070670286256596, \n",
    "    0.004140443855581318, 0.0015877804417205191, 0.0019620153822005964, 0.003115264797507788, \n",
    "    0.001622086327434346, 0.001954919555060309, 0.008488243782361429, 0.0002958579881656805, \n",
    "    0.00041841004184100416, 0.002601118480946807, 0.0011567915230317192, 0.0015629884338855893, \n",
    "    0.0021053961302819123, 0.006491398896462187, 0.006660450246436659, 0.008400537634408602, \n",
    "    0.0015903940201184843, 0.002237486854764728, 0.0027747717750215045, 0.001181655972680114, \n",
    "    0.0013020748562834878, 0.004048255202007934, 0.0010135306339634116, 0.0010640788269595013, \n",
    "    0.0022787348464132716, 0.0005319148936170213, 0.00078125, 0.0007874015748031496, 0.0018826718879433693, \n",
    "    0.0023786303846245332, 0.00291877061381746, 0.0015003750937734434, 0.0019067594622938315, \n",
    "    0.002037871809711682, 0.0063411540900443885, 0.006250390649415588, 0.006723593088146306, \n",
    "    0.00028571428571428574, 0.0007092198581560284, 0.0013328712712926186, 0.00036231884057971015, \n",
    "    0.002241800614253368, 0.0023571563266075804, 0.000111731843575419, 0.00015060240963855423, \n",
    "    0.00020920502092050208, 0.004284123040013709, 0.005326515393629487, 0.0053273666826487666, \n",
    "    0.004203976962206247, 0.005280388636603654, 0.005063547521393488, 6.63129973474801e-05, \n",
    "    9.259259259259259e-05, 0.0001388888888888889\n",
    "]\n",
    "\n",
    "\n",
    "crossbar = Crossbar(knc,SLP.weights,SLP.bias)\n",
    "crossbar.weight_mapping()\n",
    "crossbar.Gideal\n",
    "x_train = SLP.normalize_MIN_MAX(data['x_train']).reshape(data['x_train'].shape[0], -1)\n",
    "y_train = crossbar.process_data(data['y_train'],3)\n",
    "#x_test = SLP.normalize_STD(data['x_test']).reshape(data['x_test'].shape[0], -1)\n",
    "#y_test = data['y_test']\n",
    "#for i in range(10):\n",
    "#    print(x_train[i])\n",
    "#0.5 V\n",
    "Vmax = 0.5\n",
    "print(x_train[2])\n",
    "\n",
    "\n",
    "crossbar.test_conductance(x_train,y_train,Vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "650bd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849941.0530521953\n",
      "12.866666666666664\n",
      "49535908.21593823\n",
      "_________________\n",
      "y_corrected:  -105260469.84733713\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.44705882352941\n",
      "47920442.75446144\n",
      "_________________\n",
      "y_corrected:  -103645004.39511156\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "9.756862745098038\n",
      "37563346.43134847\n",
      "_________________\n",
      "y_corrected:  -93287908.12732069\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.717647058823527\n",
      "45112250.45694101\n",
      "_________________\n",
      "y_corrected:  -100836812.10974249\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "6.188235294117647\n",
      "23824341.104770057\n",
      "_________________\n",
      "y_corrected:  -79548902.87988704\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.831372549019605\n",
      "45550086.89042538\n",
      "_________________\n",
      "y_corrected:  -101274648.54033275\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.486274509803922\n",
      "44221479.781921096\n",
      "_________________\n",
      "y_corrected:  -99946041.43977562\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "24.55294117647059\n",
      "94527376.2084698\n",
      "_________________\n",
      "y_corrected:  -150251937.57593265\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "6.125490196078431\n",
      "23582776.175951093\n",
      "_________________\n",
      "y_corrected:  -79307337.95253296\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.960784313725489\n",
      "49898255.60916668\n",
      "_________________\n",
      "y_corrected:  -105622817.23399639\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.815686274509805\n",
      "49339636.71127284\n",
      "_________________\n",
      "y_corrected:  -105064198.33967945\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.20392156862745\n",
      "62384142.867496744\n",
      "_________________\n",
      "y_corrected:  -118108704.42257287\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "18.576470588235296\n",
      "71518316.73846373\n",
      "_________________\n",
      "y_corrected:  -127242878.23868573\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "7.215686274509803\n",
      "27779966.814180546\n",
      "_________________\n",
      "y_corrected:  -83504528.56649639\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "9.509803921568627\n",
      "36612184.52412382\n",
      "_________________\n",
      "y_corrected:  -92336746.2255184\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.458823529411763\n",
      "63365500.390823774\n",
      "_________________\n",
      "y_corrected:  -119090061.93761715\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.811764705882352\n",
      "53174479.95627385\n",
      "_________________\n",
      "y_corrected:  -108899041.56214628\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "5.92156862745098\n",
      "22797690.157289468\n",
      "_________________\n",
      "y_corrected:  -78522251.9383865\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.270588235294117\n",
      "43391100.33910592\n",
      "_________________\n",
      "y_corrected:  -99115662.00162789\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "8.435294117647057\n",
      "32475385.1180991\n",
      "_________________\n",
      "y_corrected:  -88199946.84648311\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.329411764705885\n",
      "62867272.72513468\n",
      "_________________\n",
      "y_corrected:  -118591834.27557287\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "14.09019607843137\n",
      "54246424.32790799\n",
      "_________________\n",
      "y_corrected:  -109970985.92750399\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "10.729411764705882\n",
      "41307602.82804237\n",
      "_________________\n",
      "y_corrected:  -97032164.50290845\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "17.48627450980392\n",
      "67321126.10023427\n",
      "_________________\n",
      "y_corrected:  -123045687.6248618\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "14.164705882352935\n",
      "54533282.68088049\n",
      "_________________\n",
      "y_corrected:  -110257844.27902524\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "8.584313725490196\n",
      "33049101.82404414\n",
      "_________________\n",
      "y_corrected:  -88773663.54608065\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "23.65098039215686\n",
      "91054880.3566972\n",
      "_________________\n",
      "y_corrected:  -146779441.74539635\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "17.211764705882352\n",
      "66264279.53665131\n",
      "_________________\n",
      "y_corrected:  -121988841.06785494\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "11.337254901960783\n",
      "43647763.07597606\n",
      "_________________\n",
      "y_corrected:  -99372324.73710555\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.533333333333333\n",
      "52102535.58463971\n",
      "_________________\n",
      "y_corrected:  -107827097.19748497\n",
      "y_train:  0.0\n",
      "_________________\n",
      "Accuracy: 0.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0, 0.0, 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Known Conductance\n",
    "knc = additional_conductance_values = [\n",
    "    2.98e-3,802e-6,5.17e-3,3.07e-3,3.80e-3,740e-6,416.67e-6,2.04e-3,24.9e-6,240.57e-6,42.37e-6,\n",
    "    380e-6,3.07e-3,2.00e-3,1.03e-3,792e-6,260e-6,1.65e-3,2.88e-3,2.30e-3,2.01e-3,1.73e-3,3.55e-3,\n",
    "    4.03e-3,5.71e-3,3.36e-3, 301e-6,14.74e-3,16.75e-3,23.42e-3,758e-6,215.57e-6,2.08e-3,20.04e-6,\n",
    "    5.74e-3,184.7e-6,143.08e-6,0.416e-3,0.730e-3,1.22e-3,692.6e-6,478.03e-6,1.5e-3,3.97e-3,1.3e-3,\n",
    "    2.04e-3,1.67e-3,11.43e-3,5.007e-3,7.93e-3,\n",
    "    0.004125923175310475, 0.004606596646397641, 0.00484214603912454, 0.0017425549340442958, \n",
    "    0.0016838702072844226, 0.004572473708276178, 0.0004716981132075472, 0.0026288808854070824, \n",
    "    0.0053769222497042695, 0.0048652330446628395, 0.0057359183205231154, 0.009719117504130625, \n",
    "    0.0032565864460872116, 0.004441384823788057, 0.006468723720809884, 0.0001269035532994924, \n",
    "    0.0004484304932735426, 0.0006711409395973154, 0.00226510827217541, 0.0022070670286256596, \n",
    "    0.004140443855581318, 0.0015877804417205191, 0.0019620153822005964, 0.003115264797507788, \n",
    "    0.001622086327434346, 0.001954919555060309, 0.008488243782361429, 0.0002958579881656805, \n",
    "    0.00041841004184100416, 0.002601118480946807, 0.0011567915230317192, 0.0015629884338855893, \n",
    "    0.0021053961302819123, 0.006491398896462187, 0.006660450246436659, 0.008400537634408602, \n",
    "    0.0015903940201184843, 0.002237486854764728, 0.0027747717750215045, 0.001181655972680114, \n",
    "    0.0013020748562834878, 0.004048255202007934, 0.0010135306339634116, 0.0010640788269595013, \n",
    "    0.0022787348464132716, 0.0005319148936170213, 0.00078125, 0.0007874015748031496, 0.0018826718879433693, \n",
    "    0.0023786303846245332, 0.00291877061381746, 0.0015003750937734434, 0.0019067594622938315, \n",
    "    0.002037871809711682, 0.0063411540900443885, 0.006250390649415588, 0.006723593088146306, \n",
    "    0.00028571428571428574, 0.0007092198581560284, 0.0013328712712926186, 0.00036231884057971015, \n",
    "    0.002241800614253368, 0.0023571563266075804, 0.000111731843575419, 0.00015060240963855423, \n",
    "    0.00020920502092050208, 0.004284123040013709, 0.005326515393629487, 0.0053273666826487666, \n",
    "    0.004203976962206247, 0.005280388636603654, 0.005063547521393488, 6.63129973474801e-05, \n",
    "    9.259259259259259e-05, 0.0001388888888888889\n",
    "]\n",
    "\n",
    "crossbar = Crossbar(knc,SLP.weights,SLP.bias)\n",
    "crossbar.weight_mapping()\n",
    "crossbar.Gideal\n",
    "x_train = SLP.normalize_MIN_MAX(data['x_train']).reshape(data['x_train'].shape[0], -1)\n",
    "y_train = crossbar.process_data(data['y_train'],3)\n",
    "#x_test = SLP.normalize_STD(data['x_test']).reshape(data['x_test'].shape[0], -1)\n",
    "#y_test = data['y_test']\n",
    "#for i in range(10):\n",
    "#    print(x_train[i])\n",
    "#0.5 V\n",
    "Vmax = 0.5\n",
    "crossbar.test_conductance(x_train,y_train,Vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b317613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossbar.weight_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72f062f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02511713, 0.02511713, 0.0250491 , 0.02499714, 0.02626501,\n",
       "       0.02717673, 0.02615365, 0.02516488, 0.02511713, 0.02511713,\n",
       "       0.02513914, 0.03569452, 0.04561366, 0.04421167, 0.04443277,\n",
       "       0.04442697, 0.04436683, 0.04435957, 0.0304537 , 0.02510321,\n",
       "       0.02552593, 0.04535715, 0.04430409, 0.04455484, 0.04449356,\n",
       "       0.04452978, 0.04444693, 0.04447534, 0.04344935, 0.02471742,\n",
       "       0.02618727, 0.04445022, 0.04444935, 0.04404591, 0.04419227,\n",
       "       0.04462375, 0.04462677, 0.04475888, 0.04398174, 0.02485805,\n",
       "       0.02500905, 0.04401043, 0.04411755, 0.04413802, 0.04454177,\n",
       "       0.04476108, 0.04450316, 0.0443245 , 0.03607953, 0.02536958,\n",
       "       0.0287466 , 0.04435769, 0.04406028, 0.04423248, 0.04454401,\n",
       "       0.0444404 , 0.04431448, 0.04447576, 0.04529355, 0.02426852,\n",
       "       0.03404029, 0.04512822, 0.04426214, 0.04386839, 0.04387574,\n",
       "       0.04408836, 0.0446004 , 0.0446528 , 0.04427622, 0.0222137 ,\n",
       "       0.0338314 , 0.0452689 , 0.04425851, 0.0444389 , 0.04431309,\n",
       "       0.04450014, 0.0444266 , 0.04439925, 0.04418764, 0.024642  ,\n",
       "       0.02792059, 0.04540286, 0.04466471, 0.04437356, 0.044352  ,\n",
       "       0.04426853, 0.04454945, 0.04465308, 0.03190176, 0.02509872,\n",
       "       0.02514001, 0.02820757, 0.04181099, 0.04476714, 0.0443129 ,\n",
       "       0.04449603, 0.04010432, 0.03042188, 0.02543308, 0.02511713])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossbar.Gideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "785bb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849941.0530521953\n",
      "-0.8309585235935919\n",
      "-3199141.3333666106\n",
      "_________________\n",
      "y_corrected:  -52525420.45558743\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-2.5095754560574886\n",
      "-9661717.574007912\n",
      "_________________\n",
      "y_corrected:  -46062844.251955144\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-13.271549434283935\n",
      "-51094683.00466136\n",
      "_________________\n",
      "y_corrected:  -4629879.042614505\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-5.427545076976027\n",
      "-20895728.609141342\n",
      "_________________\n",
      "y_corrected:  -34828833.265432484\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-27.547637364584308\n",
      "-106056780.0045077\n",
      "_________________\n",
      "y_corrected:  50332217.640617765\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-4.972592824252169\n",
      "-19144189.25420119\n",
      "_________________\n",
      "y_corrected:  -36580372.60879492\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-6.353137591138359\n",
      "-24459205.2278127\n",
      "_________________\n",
      "y_corrected:  -31265356.666975528\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "45.91930744596158\n",
      "176786626.86393285\n",
      "_________________\n",
      "y_corrected:  -232511187.597026\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-27.798645504018175\n",
      "-107023146.5451644\n",
      "_________________\n",
      "y_corrected:  51298584.17541431\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-0.45444631444281314\n",
      "-1749591.522381653\n",
      "_________________\n",
      "y_corrected:  -53974970.24029272\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-1.0349026368835959\n",
      "-3984314.147650125\n",
      "_________________\n",
      "y_corrected:  -51740247.62933343\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "12.51953689254446\n",
      "48199479.04780843\n",
      "_________________\n",
      "y_corrected:  -103924040.53143759\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "22.010782164887075\n",
      "84740213.86638783\n",
      "_________________\n",
      "y_corrected:  -140464775.13057622\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-23.43737908135499\n",
      "-90232527.90125532\n",
      "_________________\n",
      "y_corrected:  34507965.62858005\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-14.259893983304744\n",
      "-54899751.25849693\n",
      "_________________\n",
      "y_corrected:  -824810.8104708493\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.539257458994514\n",
      "52125343.119226135\n",
      "_________________\n",
      "y_corrected:  -107849904.56972072\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "2.949851576628824\n",
      "11356754.685274053\n",
      "_________________\n",
      "y_corrected:  -67081316.37211093\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-28.61442195717818\n",
      "-110163837.80229843\n",
      "_________________\n",
      "y_corrected:  54439275.4144857\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-7.215978070442232\n",
      "-27781090.211319916\n",
      "_________________\n",
      "y_corrected:  -27943471.702140175\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-18.558408371109476\n",
      "-71448778.2672419\n",
      "_________________\n",
      "y_corrected:  15724216.090304524\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "13.021553171412183\n",
      "50132212.12912177\n",
      "_________________\n",
      "y_corrected:  -105856773.59419718\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "4.063700195366545\n",
      "15645006.209437888\n",
      "_________________\n",
      "y_corrected:  -71369567.87116626\n",
      "y_train:  1.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-9.38092327305922\n",
      "-36116001.624483466\n",
      "_________________\n",
      "y_corrected:  -19608560.338358507\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "17.649515742223873\n",
      "67949595.22247867\n",
      "_________________\n",
      "y_corrected:  -123674156.58430004\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "4.361772360944246\n",
      "16792566.47646765\n",
      "_________________\n",
      "y_corrected:  -72517128.1323904\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-17.96226403995408\n",
      "-69153657.73318239\n",
      "_________________\n",
      "y_corrected:  13429095.581637792\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "42.31106544159994\n",
      "162895107.84199363\n",
      "_________________\n",
      "y_corrected:  -218619668.6600413\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "16.55135513220076\n",
      "63721741.60710586\n",
      "_________________\n",
      "y_corrected:  -119446302.99523449\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "-6.949281922293764\n",
      "-26754325.76187224\n",
      "_________________\n",
      "y_corrected:  -28970236.146017298\n",
      "y_train:  0.0\n",
      "_________________\n",
      "3849941.0530521953\n",
      "1.8360029578910897\n",
      "7068503.161110167\n",
      "_________________\n",
      "y_corrected:  -62793064.8758414\n",
      "y_train:  0.0\n",
      "_________________\n",
      "Accuracy: 0.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0, 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = SLP.normalize_STD(data['x_train']).reshape(data['x_train'].shape[0], -1)\n",
    "y_train = crossbar.process_data(data['y_train'],3)\n",
    "#x_test = SLP.normalize_STD(data['x_test']).reshape(data['x_test'].shape[0], -1)\n",
    "#y_test = data['y_test']\n",
    "#for i in range(10):\n",
    "#    print(x_train[i])\n",
    "#0.5 V\n",
    "Vmax = 0.5\n",
    "crossbar.test_conductance(x_train,y_train,Vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bed9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24a51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
